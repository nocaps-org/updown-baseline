
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120523111-2"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-120523111-2');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Inconsolata&display=swap" rel="stylesheet">


    <title>updown.modules.updown_cell &#8212; updown 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="updown.utils" href="utils.html" />
    <link rel="prev" title="updown.modules.attention" href="modules.attention.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-updown.modules.updown_cell">
<span id="updown-modules-updown-cell"></span><h1>updown.modules.updown_cell<a class="headerlink" href="#module-updown.modules.updown_cell" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="updown.modules.updown_cell.UpDownCell">
<em class="property">class </em><code class="sig-prename descclassname">updown.modules.updown_cell.</code><code class="sig-name descname">UpDownCell</code><span class="sig-paren">(</span><em class="sig-param">image_feature_size: int</em>, <em class="sig-param">embedding_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">attention_projection_size: int</em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/nocaps-org/updown-baseline/blob/master/updown/modules/updown_cell.py#L11-L200"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#updown.modules.updown_cell.UpDownCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>The basic computation unit of <a class="reference internal" href="models.updown_captioner.html#updown.models.updown_captioner.UpDownCaptioner" title="updown.models.updown_captioner.UpDownCaptioner"><code class="xref py py-class docutils literal notranslate"><span class="pre">UpDownCaptioner</span></code></a>.</p>
<p>The architecture (<a class="reference external" href="https://arxiv.org/abs/1707.07998">Anderson et al. 2017 (Fig. 3)</a>)
is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>                                h2 (t)
                                 .^.
                                  |
                   +--------------------------------+
    h2 (t-1) ----&gt; |         Language LSTM          | ----&gt; h2 (t)
                   +--------------------------------+
                     .^.         .^.
                      |           |
bottom-up     +----------------+  |
features  --&gt; | BUTD Attention |  |
              +----------------+  |
                     .^.          |
                      |___________|
                                  |
                   +--------------------------------+
    h1 (t-1) ----&gt; |         Attention LSTM         | ----&gt; h1 (t)
                   +--------------------------------+
                                 .^.
                __________________|__________________
                |                 |                  |
                |             mean pooled        input token
            h2 (t-1)           features           embedding
</pre></div>
</div>
<p>If <a class="reference internal" href="models.updown_captioner.html#updown.models.updown_captioner.UpDownCaptioner" title="updown.models.updown_captioner.UpDownCaptioner"><code class="xref py py-class docutils literal notranslate"><span class="pre">UpDownCaptioner</span></code></a> is analogous to an
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM" title="(in PyTorch vmaster (1.1.0 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTM</span></code></a>, then this class would be analogous to <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell" title="(in PyTorch vmaster (1.1.0 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMCell</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image_feature_size: int</strong></dt><dd><p>Size of the bottom-up image features.</p>
</dd>
<dt><strong>embedding_size: int</strong></dt><dd><p>Size of the word embedding input to the captioner.</p>
</dd>
<dt><strong>hidden_size: int</strong></dt><dd><p>Size of the hidden / cell states of attention LSTM and language LSTM of the captioner.</p>
</dd>
<dt><strong>attention_projection_size: int</strong></dt><dd><p>Size of the projected image and textual features before computing bottom-up top-down
attention weights.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="method">
<dt id="updown.modules.updown_cell.UpDownCell.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">self, image_features:torch.Tensor, token_embedding:torch.Tensor, states:Union[Dict[str, torch.Tensor], NoneType]=None</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, Dict[str, torch.Tensor]]<a class="reference external" href="https://github.com/nocaps-org/updown-baseline/blob/master/updown/modules/updown_cell.py#L85-L160"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#updown.modules.updown_cell.UpDownCell.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Given image features, input token embeddings of current time-step and LSTM states,
predict output token embeddings for next time-step and update states. This behaves
very similar to <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell" title="(in PyTorch vmaster (1.1.0 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTMCell</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image_features: torch.Tensor</strong></dt><dd><p>A tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_boxes,</span> <span class="pre">image_feature_size)</span></code>. <code class="docutils literal notranslate"><span class="pre">num_boxes</span></code> for
each instance in a batch might be different. Instances with lesser boxes are padded
with zeros up to <code class="docutils literal notranslate"><span class="pre">num_boxes</span></code>.</p>
</dd>
<dt><strong>token_embedding: torch.Tensor</strong></dt><dd><p>A tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">embedding_size)</span></code> containing token embeddings for a
particular time-step.</p>
</dd>
<dt><strong>states: Dict[str, torch.Tensor], optional (default = None)</strong></dt><dd><p>A dict with keys <code class="docutils literal notranslate"><span class="pre">{&quot;h1&quot;,</span> <span class="pre">&quot;c1&quot;,</span> <span class="pre">&quot;h2&quot;,</span> <span class="pre">&quot;c2&quot;}</span></code> of LSTM states: (h1, c1) for Attention
LSTM and (h2, c2) for Language LSTM. If not provided (at first time-step), these are
initialized as zeros.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>Tuple[torch.Tensor, Dict[str, torch.Tensor]]</dt><dd><p>A tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">hidden_state)</span></code> with output token embedding, which
is the updated state “h2”, and updated states (h1, c1), (h2, c2).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="attribute">
<dt id="updown.modules.updown_cell.UpDownCell._average_image_features">
<code class="sig-name descname">_average_image_features</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">image_features:torch.Tensor</em><span class="sig-paren">)</span> &#x2192; Tuple[torch.Tensor, torch.Tensor]<a class="headerlink" href="#updown.modules.updown_cell.UpDownCell._average_image_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform mean pooling of bottom-up image features, while taking care of variable
<code class="docutils literal notranslate"><span class="pre">num_boxes</span></code> in case of adaptive features.</p>
<p>For a single training/evaluation instance, the image features remain the same from first
time-step to maximum decoding steps. To keep a clean API, we use LRU cache – which would
maintain a cache of last 10 return values because on call signature, and not actually
execute itself if it is called with the same image features seen at least once in last
10 calls. This saves some computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>image_features: torch.Tensor</strong></dt><dd><p>A tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_boxes,</span> <span class="pre">image_feature_size)</span></code>. <code class="docutils literal notranslate"><span class="pre">num_boxes</span></code> for
each instance in a batch might be different. Instances with lesser boxes are padded
with zeros up to <code class="docutils literal notranslate"><span class="pre">num_boxes</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt>Tuple[torch.Tensor, torch.Tensor]</dt><dd><p>Averaged image features of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">image_feature_size)</span></code> and a binary
mask of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_boxes)</span></code> which is zero for padded features.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">updown</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="config.html">updown.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">updown.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">updown.models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">updown.modules</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="modules.attention.html">updown.modules.attention</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">updown.modules.updown_cell</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">updown.utils</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="modules.html">updown.modules</a><ul>
      <li>Previous: <a href="modules.attention.html" title="previous chapter">updown.modules.attention</a></li>
      <li>Next: <a href="utils.html" title="next chapter">updown.utils</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, nocaps team.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.1.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/updown/modules.updown_cell.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>